# Local Qdrant RAG Agent ğŸ¤–

Ein vollstÃ¤ndig lokaler RAG-Assistent (Retrieval Augmented Generation) fÃ¼r deutsche Unternehmen. **100% GDPR-konform** - alle Daten bleiben auf Ihrer Infrastruktur.

![Python 3.10+](https://img.shields.io/badge/Python-3.10+-blue.svg)
![License MIT](https://img.shields.io/badge/License-MIT-green.svg)
![Local First](https://img.shields.io/badge/Local-First-orange.svg)

## âœ¨ Features

### Kernfunktionen
- ğŸ”’ **100% Lokal** - Keine Cloud-AbhÃ¤ngigkeiten, keine Daten verlassen Ihr Netzwerk
- ğŸ‡©ğŸ‡ª **Deutschsprachig** - Optimiert fÃ¼r deutsche Texte und Antworten
- ğŸ“„ **Docling-powered** - IBM's Document Understanding fÃ¼r komplexe Dokumente
- ğŸ” **Hybrid Search** - Kombiniert semantische Suche mit Volltextsuche (RRF-Merge)
- âš¡ **Streaming** - Antworten werden Wort fÃ¼r Wort angezeigt
- ğŸŒ **OpenAI-kompatible API** - Integration mit OpenWebUI, Continue.dev und anderen Tools

### Dokumentverarbeitung
| Format | Features |
|--------|----------|
| PDF | Komplexe Layouts, Tabellen, OCR fÃ¼r gescannte Docs |
| Word (.docx) | Formatierung, Tabellen, Styles |
| PowerPoint (.pptx) | Folien, Notizen |
| Excel (.xlsx) | Sheets, Formeln (als Werte) |
| HTML/Markdown | Struktur-erhaltend |
| Images | OCR fÃ¼r PNG, JPG, TIFF |

### Intelligente Befehle
- **Chat-basierte Indexierung**: `indexiere ~/Desktop/Dokumente`
- **Dateisystem-Navigation**: `ls`, `cd`, `tree`, `pwd`
- **Datei-Operationen**: Erstellen, Verschieben, Kopieren, LÃ¶schen
- **Multi-Collection**: Mehrere Wissensdatenbanken verwalten
- **ERP-Ã¤hnliche Organisation**: Dokumente automatisch nach Kunden/Projekten sortieren

## ğŸš€ Quickstart

### Voraussetzungen

- Python 3.10+
- Docker (fÃ¼r Qdrant)
- [Ollama](https://ollama.ai) installiert
- ~8GB freier Speicherplatz

### Installation

```bash
# 1. Repository klonen
git clone https://github.com/YOUR_USERNAME/local-qdrant-rag.git
cd local-qdrant-rag

# 2. Virtual Environment erstellen
python -m venv venv
source venv/bin/activate  # Linux/macOS
# oder: venv\Scripts\activate  # Windows

# 3. Dependencies installieren
pip install -r requirements.txt

# 4. Konfiguration
cp .env.example .env
# Optional: .env anpassen

# 5. Qdrant starten
docker compose up -d

# 6. Ollama-Modell laden
ollama pull qwen2.5:32b
# Oder fÃ¼r schwÃ¤chere Hardware: ollama pull qwen2.5:7b
```

### Erster Test

```bash
# Health Check
python -m src.cli health

# Dokumente indexieren
python -m src.cli ingest -d ./documents -r

# Chat starten
python -m src.cli chat --show-sources
```

## ğŸ’¬ Chat-Befehle

### Indexierung
```
indexiere /pfad/zum/ordner
indexiere ~/Desktop -r          # rekursiv
fÃ¼ge /pfad hinzu
```

### Wissensdatenbanken
```
erstelle wissensdatenbank projekt-2025
zeige alle wissensdatenbanken
wechsel zu projekt-2025
lÃ¶sche wissensdatenbank test
```

### Dateisystem
```
ls /pfad                        # Verzeichnis anzeigen
cd ~/Desktop                    # Navigieren
pwd                             # Aktuelles Verzeichnis
tree /pfad                      # Baumstruktur
erstelle ordner neuer_ordner
verschiebe alt.txt nach neu.txt
```

### Organisation
```
organisiere ~/Desktop nach themen           # Themen-basiert
organisiere ~/Dokumente mit wissen          # ERP-Ã¤hnlich (Kunden/Projekte)
rÃ¤ume auf den desktop                       # Quick-Tidy
finde Ã¤hnliche dokumente zu /pfad/doc.pdf
```

### Fragen
```
Was macht TimeSkipCom?
ErklÃ¤re mir die RAG-Architektur
Fasse den Vertrag zusammen
```

## ğŸŒ API Server (OpenWebUI Integration)

Der RAG-Agent kann als OpenAI-kompatibler API-Server gestartet werden, um mit Tools wie **OpenWebUI**, **Continue.dev** oder anderen OpenAI-kompatiblen Clients zu funktionieren.

### Server starten

```bash
# Via CLI (findet automatisch freien Port)
python -m src.cli serve

# Mit bestimmtem Port
python -m src.cli serve --port 9000

# Oder direkt via uvicorn
python -m uvicorn src.api:app --host 0.0.0.0 --port 8001
```

> **Hinweis:** Der Server prÃ¼ft automatisch ob der gewÃ¼nschte Port frei ist und sucht bei Bedarf einen freien Port. Default-Port ist 8001.

### OpenWebUI konfigurieren

1. Ã–ffne OpenWebUI Settings â†’ Connections
2. FÃ¼ge eine neue OpenAI-Connection hinzu:
   - **Base URL**: `http://localhost:PORT/v1` (PORT aus Server-Output)
   - **API Key**: beliebig (z.B. `local-rag`)
3. WÃ¤hle das Model `local-rag` aus

### API Endpoints

| Endpoint | Methode | Beschreibung |
|----------|---------|--------------|
| `/v1/chat/completions` | POST | OpenAI-kompatibler Chat (mit RAG) |
| `/v1/models` | GET | VerfÃ¼gbare Modelle |
| `/v1/rag/search` | POST | Direkte RAG-Suche ohne LLM |
| `/v1/rag/collections` | GET | Qdrant Collections auflisten |
| `/health` | GET | Health-Check |
| `/docs` | GET | Swagger UI Dokumentation |

### Beispiel: Chat mit cURL

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "local-rag",
    "messages": [{"role": "user", "content": "Was ist RAG?"}],
    "stream": false
  }'
```

### RAG deaktivieren

Falls du die RAG-Suche fÃ¼r einzelne Anfragen deaktivieren mÃ¶chtest:

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "local-rag",
    "messages": [{"role": "user", "content": "Hallo!"}],
    "use_rag": false
  }'
```

## âš™ï¸ Konfiguration

Alle Einstellungen Ã¼ber `.env` oder Umgebungsvariablen:

| Variable | Default | Beschreibung |
|----------|---------|--------------|
| `QDRANT_URL` | `http://localhost:6333` | Qdrant Server |
| `OLLAMA_MODEL` | `qwen2.5:32b` | LLM Modell |
| `EMBEDDING_MODEL` | `BAAI/bge-m3` | Embedding Modell |
| `EMBEDDING_DIMENSION` | `1024` | Embedding Dimension |
| `CHUNK_SIZE` | `1000` | Max Tokens pro Chunk |
| `TOP_K` | `10` | Suchergebnisse |
| `RRF_K` | `60` | RRF Konstante |
| `MIN_SCORE` | `0.01` | Minimaler Relevanz-Score |
| `RETRIEVAL_STRATEGY` | `hybrid_rrf` | Such-Strategie |

## ğŸ—ï¸ Architektur

```
Chat-Eingabe
    â”‚
    â”œâ”€â–º Greeting? â†’ Kurze Antwort
    â”‚
    â”œâ”€â–º Meta-Frage? â†’ Selbstbeschreibung + Collection-Info
    â”‚
    â”œâ”€â–º Filesystem-Befehl? â†’ Navigator/Operations/Organizer
    â”‚       â”œâ”€â–º ls, cd, pwd, tree
    â”‚       â”œâ”€â–º mkdir, mv, cp, rm
    â”‚       â””â”€â–º organisiere (Themen/Wissen)
    â”‚
    â”œâ”€â–º Collection-Befehl? â†’ Collection Manager
    â”‚
    â”œâ”€â–º Index-Befehl? â†’ Docling Pipeline
    â”‚
    â””â”€â–º Inhaltsfrage? â†’ RAG (Hybrid-Suche â†’ Ollama)
```

### Tech Stack

- **Vector DB**: Qdrant (lokal via Docker)
- **LLM**: Ollama (qwen2.5, llama3.1)
- **Embeddings**: BGE-M3 (multilingual, 1024 dim)
- **Dokumente**: Docling (IBM)
- **Suche**: Hybrid RRF (Semantic + Fulltext)
- **API**: FastAPI (OpenAI-kompatibel)

## ğŸ’» Hardware-Empfehlungen

| Setup | RAM | Modell | Bemerkung |
|-------|-----|--------|-----------|
| Minimal | 16GB | qwen2.5:7b | Funktional |
| Standard | 32GB | qwen2.5:14b | Gute Balance |
| Empfohlen | 64GB | qwen2.5:32b | Beste QualitÃ¤t |
| High-End | 128GB+ | llama3.1:70b | Maximum |

> Apple Silicon (M1/M2/M3/M4) nutzt automatisch MPS fÃ¼r GPU-Beschleunigung.

## ğŸ§ª Tests

```bash
# Pattern-Tests
python tests/test_patterns.py

# Filesystem-Tests
python test_filesystem_functions.py

# System Health Check
python system_health_check.py
```

## ğŸ“ Projektstruktur

```
local-qdrant-rag/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cli.py                  # CLI + Chat Interface
â”‚   â”œâ”€â”€ api.py                  # OpenAI-kompatible REST API
â”‚   â”œâ”€â”€ settings.py             # Konfiguration
â”‚   â”œâ”€â”€ tools.py                # Search Tools
â”‚   â”œâ”€â”€ ingestion/              # Docling Pipeline
â”‚   â”‚   â”œâ”€â”€ document_loader.py
â”‚   â”‚   â”œâ”€â”€ chunker.py
â”‚   â”‚   â”œâ”€â”€ embedder.py
â”‚   â”‚   â””â”€â”€ ingest.py
â”‚   â”œâ”€â”€ retrieval/              # Hybrid Search
â”‚   â”‚   â”œâ”€â”€ semantic.py
â”‚   â”‚   â”œâ”€â”€ fulltext.py
â”‚   â”‚   â””â”€â”€ hybrid_rrf.py
â”‚   â”œâ”€â”€ vectorstore/            # Qdrant Integration
â”‚   â”‚   â”œâ”€â”€ qdrant_client.py
â”‚   â”‚   â”œâ”€â”€ schema.py
â”‚   â”‚   â””â”€â”€ collection_manager.py
â”‚   â”œâ”€â”€ filesystem/             # Dateisystem-Operationen
â”‚   â”‚   â”œâ”€â”€ navigator.py
â”‚   â”‚   â”œâ”€â”€ operations.py
â”‚   â”‚   â”œâ”€â”€ organizer.py
â”‚   â”‚   â””â”€â”€ knowledge_organizer.py
â”‚   â””â”€â”€ providers/
â”‚       â””â”€â”€ ollama_provider.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_patterns.py
â”œâ”€â”€ documents/                  # Beispiel-Dokumente
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â””â”€â”€ .env.example
```

## ğŸ”§ Troubleshooting

### Qdrant nicht erreichbar
```bash
docker ps                    # Container Status
docker compose down
docker compose up -d
docker compose logs qdrant   # Logs prÃ¼fen
```

### Ollama nicht erreichbar
```bash
ollama serve                 # Service starten
ollama list                  # Modelle prÃ¼fen
ollama pull qwen2.5:32b      # Modell laden
```

### Docling Download langsam
```bash
# Manueller Download der Modelle
python -c "from docling.document_converter import DocumentConverter; DocumentConverter()"
```

### Out of Memory
- Kleineres LLM: `OLLAMA_MODEL=qwen2.5:7b`
- `CHUNK_SIZE` reduzieren
- `TOP_K` reduzieren

## ğŸ“ Changelog

Siehe [CHANGELOG.md](CHANGELOG.md) fÃ¼r alle Ã„nderungen.

## ğŸ¤ Contributing

Contributions sind willkommen! Bitte:

1. Fork das Repository
2. Erstelle einen Feature-Branch (`git checkout -b feature/AmazingFeature`)
3. Commit deine Ã„nderungen (`git commit -m 'Add some AmazingFeature'`)
4. Push zum Branch (`git push origin feature/AmazingFeature`)
5. Ã–ffne einen Pull Request

## ğŸ“„ Lizenz

MIT License - siehe [LICENSE](LICENSE) fÃ¼r Details.

## ğŸ™ Attribution

UrsprÃ¼nglich basierend auf [MongoDB-RAG-Agent](https://github.com/coleam00/MongoDB-RAG-Agent) von Cole Medin, vollstÃ¤ndig umgeschrieben fÃ¼r lokalen Betrieb mit Qdrant, Docling und Ollama.

---

Entwickelt von **TimeSkipCom** fÃ¼r GDPR-konforme AI-LÃ¶sungen in deutschen Unternehmen.
